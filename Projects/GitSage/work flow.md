# work flow

## âš™ï¸ **1ï¸âƒ£ Indexing Workflow (One-time setup per repo)**

> Purpose: Prepare â€œmemoryâ€ for the repo â†’ embeddings â†’ FAISS index.
> 

---

### ğŸ©µ **Step 1 â€” Input**

**Component:** Web UI / n8n webhook / API endpoint

**Action:** User submits repo URL.

**Example:**

`POST /analyze_repo`

Payload: `{ "repo_url": "https://github.com/user/project" }`

---

### âš™ï¸ **Step 2 â€” Clone Repository**

**Component:** GitPython

**Action:** Clone the repo locally into a workspace folder.

**Output:** Local repo files on disk.

---

### ğŸ“ **Step 3 â€” File Parsing**

**Component:** Python script

**Action:** Traverse all directories and filter files:

- Skip `.git/`, binaries, and large media files.
- Collect `.py`, `.js`, `.md`, `.json`, etc.

**Output:** A list of files + their content + metadata.

---

### âœ‚ï¸ **Step 4 â€” Chunking**

**Component:** LangChainâ€™s `RecursiveCharacterTextSplitter`

**Action:** Break long files into smaller chunks (e.g. 800â€“1,000 tokens each).

**Goal:** Make each piece small enough for LLM input later.

**Output:** Chunks of text with metadata (file name, path).

---

### ğŸ§¬ **Step 5 â€” Embedding Generation**

**Component:** OpenAI / Hugging Face Embedding API

**Action:** Convert each text chunk into a **vector embedding**.

**Each vector:** 1,536 floats (â‰ˆ6 KB per chunk).

**Output:** (chunk_id, vector, metadata)

---

### ğŸ’¾ **Step 6 â€” Store in FAISS**

**Component:** FAISS index (HNSW or IVF Flat)

**Action:** Add all vectors to FAISS index.

**Save index to disk:** `repo_name.index`

**Output:** Searchable FAISS vector memory for that repo.

---

### ğŸ—ƒï¸ **Step 7 â€” Store Metadata in MySQL**

**Component:** MySQL

**Action:** Insert record in database:

| Field | Example |
| --- | --- |
| repo_name | "requests" |
| repo_url | "[https://github.com/psf/requests](https://github.com/psf/requests)" |
| index_path | "/data/requests.index" |
| total_files | 124 |
| total_chunks | 960 |
| analyzed_at | timestamp |

**Output:** DB entry to identify the repo.

---

### âœ… **Step 8 â€” Confirm Completion**

**Component:** API response or n8n node

**Output:**

`{"status": "indexed", "repo_id": 12}`

Now your repoâ€™s â€œmemoryâ€ is ready in FAISS.

---

## ğŸ’¬ **2ï¸âƒ£ Query Workflow (Whenever user asks a question)**

> Purpose: Retrieve relevant code context â†’ send to LLM â†’ generate explanation.
> 

---

### ğŸ’¡ **Step 1 â€” User Question**

**Component:** Web UI / CLI / API

**Example:**

`"How does this repo handle database connections?"`

---

### âš™ï¸ **Step 2 â€” Fetch Repo Context**

**Component:** Backend API

**Action:** Identify which repo the question refers to (via `repo_id`).

---

### ğŸ§¬ **Step 3 â€” Embed the Question**

**Component:** Same embedding model (for consistency)

**Action:** Convert user question â†’ vector embedding.

**Output:** 1 question vector.

---

### ğŸ” **Step 4 â€” Semantic Search (FAISS Retrieval)**

**Component:** FAISS

**Action:**

Compare question vector with stored repo vectors.

Find **Top N (e.g., 5)** closest matches.

**Output:** The most relevant chunks of code/docs + metadata (file paths).

---

### ğŸ“¦ **Step 5 â€” Context Assembly**

**Component:** Backend logic / LangChain

**Action:** Combine those top chunks into a single context string.

Example:

```
Context:
[Chunk1: auth.py lines 12-60]
[Chunk2: db_connection.py lines 5-40]
Question: "How does this repo handle database connections?"

```

---

### ğŸ¤– **Step 6 â€” Send to LLM**

**Component:** OpenAI GPT API (ChatCompletion endpoint)

**Action:**

Prompt the LLM with:

```
"You are an AI assistant. Based on the provided code context, answer the question precisely."

<context_here>
<question_here>

```

**Output:** Natural language explanation.

---

### ğŸ§  **Step 7 â€” Post-process & Summarize**

**Component:** Backend logic

**Action:**

Optionally clean up LLM output, add citations (file names, line numbers).

**Output:** Final response object:

```json
{
  "answer": "The repo connects to MySQL using SQLAlchemy...",
  "sources": ["db_connection.py", "config.py"]
}

```

---

### ğŸ§­ **Step 8 â€” Visualization (Optional)**

**Component:** D3.js / Mermaid.js / Graphviz

**Action:**

Generate a mind map showing:

- Repo hierarchy
- Files referenced in the answer
- Relationships (imports, dependencies)

**Output:** Interactive graph.

---

### ğŸ’¾ **Step 9 â€” Store Query Log**

**Component:** MySQL

**Action:** Store each Q&A for later analysis (query history, accuracy, usage).

---

### ğŸ©µ **Step 10 â€” Return Output to User**

**Component:** Web UI / API response / n8n output node

**Output:**

- LLM answer
- Source files
- Mind map link

---

## ğŸ”„ **Combined Data Flow Summary (Both Workflows)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                USER / CLIENT                 â”‚
â”‚  â€¢ Input repo URL  â€¢ Ask repo questions      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚        BACKEND API       â”‚
   â”‚(FastAPI / n8n Webhook)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Indexing     â”‚  <â”€â”€â”€ Repo URL
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚    â”‚
           â–¼    â–¼
     GitPython   File Parser
           â”‚
           â–¼
     Chunking â†’ Embeddings â†’ FAISS Index â†’ MySQL Metadata
           â”‚
           â–¼
     (Repo memory ready)

Query Stage:
User Question â†’ Embedding â†’ FAISS Search â†’ Context â†’ LLM â†’ Response â†’ Visualization â†’ User

```

---

## âš¡ **Data Movement Summary Table**

| Step | From | To | Data Type |
| --- | --- | --- | --- |
| 1 | User | Backend | JSON (repo_url / question) |
| 2 | Backend | GitPython | URL (string) |
| 3 | GitPython | Disk | Repo files |
| 4 | Files | Text Splitter | Plain text |
| 5 | Text | Embedding Model | Vector (float32 array) |
| 6 | Embeddings | FAISS | Vector index |
| 7 | Metadata | MySQL | Structured rows |
| 8 | User Query | Embedding Model | Vector |
| 9 | FAISS | Backend | Top-N chunks |
| 10 | Backend | LLM | Context + question |
| 11 | LLM | Backend | Answer text |
| 12 | Backend | UI | JSON (answer + sources) |

---

## ğŸ§  **Pro Optimization Ideas**

- Cache embeddings for repeated questions (Redis).
- Store FAISS index path in DB for multi-repo queries.
- Use batch embedding for multiple files at once (to cut cost).
- Compress FAISS index with Product Quantization if it grows large.
- Add auto-suggested questions using LLM (e.g., â€œWhat to ask next?â€).

---

âœ… **In short:**

> RepoMindâ€™s workflow turns repos â†’ embeddings â†’ searchable memory â†’ intelligent answers â€” all orchestrated by your backend and powered by FAISS + LLM synergy.
>