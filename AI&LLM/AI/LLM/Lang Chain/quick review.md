## ğŸ§  **LangChain Cheat Sheet (2025 Quick Review)**

### ğŸ”¹ 1. **What is LangChain?**

LangChain helps you **build LLM-powered applications** that can:

- Use **external data** (databases, APIs, documents)
- **Reason** with tools (calculators, search)
- Manage **multi-step chains** (question â†’ retrieval â†’ summarization)
- Maintain **memory** across conversations

---

### ğŸ”¹ 2. **Core Concepts**

| Concept | Description | Example |
| --- | --- | --- |
| **LLM** | Interface to any language model | `from langchain.llms import OpenAI` |
| **PromptTemplate** | Structure input prompts | `PromptTemplate("Translate {text} to French")` |
| **Chain** | Combines components (Prompt â†’ LLM â†’ Output) | `LLMChain(llm=llm, prompt=prompt)` |
| **Tool** | External utility (e.g., calculator, API) | `Tool(name="Calculator", func=math_tool)` |
| **Agent** | LLM that decides which tool to use | `initialize_agent(tools, llm, agent_type="zero-shot-react-description")` |
| **Memory** | Stores past context | `ConversationBufferMemory()` |
| **Retriever** | Fetches relevant chunks from vector DB | `Chroma.from_documents(docs, embeddings)` |

---

### ğŸ”¹ 3. **Common Workflows**

### ğŸ§© a. **Simple LLM Chain**

```python
from langchain import OpenAI, PromptTemplate, LLMChain

llm = OpenAI(model="gpt-3.5-turbo")
prompt = PromptTemplate(input_variables=["topic"], template="Explain {topic} simply.")
chain = LLMChain(llm=llm, prompt=prompt)

response = chain.run("quantum computing")
print(response)

```

### ğŸ§© b. **Document Q&A (Retrieval-Augmented Generation)**

```python
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import OpenAI
from langchain.document_loaders import TextLoader

# Load and index documents
loader = TextLoader("notes.txt")
docs = loader.load()
db = Chroma.from_documents(docs, OpenAIEmbeddings())

qa = RetrievalQA.from_chain_type(llm=OpenAI(), retriever=db.as_retriever())
qa.run("What are LangChain components?")

```

### ğŸ§© c. **Tool-Using Agent**

```python
from langchain.agents import initialize_agent, load_tools
from langchain.llms import OpenAI

llm = OpenAI(temperature=0)
tools = load_tools(["serpapi", "llm-math"], llm=llm)
agent = initialize_agent(tools, llm, agent_type="zero-shot-react-description")

agent.run("What's 12% of the population of France?")

```

---

### ğŸ”¹ 4. **Memory Types**

| Memory Type | Purpose |
| --- | --- |
| `ConversationBufferMemory` | Stores entire chat history |
| `ConversationSummaryMemory` | Summarizes past interactions |
| `ConversationBufferWindowMemory` | Keeps last *n* messages |
| `VectorStoreRetrieverMemory` | Semantic memory using embeddings |

---

### ğŸ”¹ 5. **Vector Stores**

Used for **semantic search** and **retrieval**.

- **Common Stores**: Chroma, FAISS, Pinecone, Milvus
- **Workflow**:
    1. Load docs â†’
    2. Embed them â†’
    3. Store in vector DB â†’
    4. Retrieve with similarity search.

---

### ğŸ”¹ 6. **Popular Integrations**

- ğŸ§¾ **OpenAI** â€” GPT models
- ğŸ” **SerpAPI** â€” web search
- ğŸ“š **Chroma / Pinecone** â€” vector DBs
- ğŸ—£ **LangServe** â€” deploy LangChain apps
- âš™ï¸ **LangGraph** â€” visualize chain/agent logic

---

### ğŸ”¹ 7. **Quick Tips**

âœ… Use **`LangSmith`** for debugging & monitoring chains

âœ… Prefer **retrievers** over static prompts for large data

âœ… Combine **memory + tools + retrieval** for advanced chatbots

âœ… For deployment: use **LangServe + FastAPI**

---

### âš¡ **Minimal End-to-End Example**

```python
from langchain import OpenAI, LLMChain, PromptTemplate

prompt = PromptTemplate.from_template("Summarize: {text}")
llm = OpenAI(temperature=0)
chain = LLMChain(prompt=prompt, llm=llm)

print(chain.run("LangChain simplifies building LLM apps."))

```